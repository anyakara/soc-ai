{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "@author: akarra1\n",
        "@author-email: akarra1@uci.edu\n",
        "@project: Socrates AI Chatbot\n",
        "@purpose: Train DistilBERT for Socrates\n",
        "'''\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "C8MHWkJQUM_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gk8X-q4Qj6C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "posts_train_df = pd.read_csv(\"posts_train.csv\")\n",
        "posts_test_df = pd.read_csv(\"posts_test.csv\")\n",
        "posts_val_df = pd.read_csv(\"posts_val.csv\")"
      ],
      "metadata": {
        "id": "e0VMTSk5ShKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(posts_train_df.index))\n",
        "print(len(posts_test_df.index))\n",
        "print(len(posts_val_df.index))"
      ],
      "metadata": {
        "id": "sselEjw9ov19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_name = 'distilbert-base-uncased' # to be moved to the configuration file\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(transformer_name)"
      ],
      "metadata": {
        "id": "IHs4rsjwgYp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing train folder\n",
        "def tokenize_text(data_group):\n",
        "  post_group_tokens = list()\n",
        "  post_group_targetLabels = list()\n",
        "  for index, row in posts_train_df.iterrows():\n",
        "      #encoded_text = tokenizer.encode(posts_train_df.iloc['post'][i], max_length=512, pad_to_max_length=True)\n",
        "      encoded_text = tokenizer.encode(row['post'], max_length=512, pad_to_max_length=True)\n",
        "      if len(encoded_text) != 512:\n",
        "        encoded_text = encoded_text[:512]\n",
        "      post_group_tokens.append(encoded_text)\n",
        "      post_group_targetLabels.append(row['class_id'])\n",
        "  print(f\"finish tokenizing {data_group}\")\n",
        "  print(\"returning tokens and target labels\")\n",
        "  return post_group_tokens, post_group_targetLabels"
      ],
      "metadata": {
        "id": "P5W4l6I3Tpqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# post train\n",
        "post_train_tokens, post_train_targetLabels = list(), list()\n",
        "\n",
        "post_train_tokens, post_train_targetLabels = tokenize_text(posts_train_df)\n",
        "print(\"post_train_tokens last 5 of dataframe\")\n",
        "print(post_train_tokens[:5])\n",
        "print('post_train_targetLabels last 5 of dataframe')\n",
        "print(post_train_targetLabels[:5])"
      ],
      "metadata": {
        "id": "Leh66IcoqIu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_train_token_series = pd.Series(post_train_tokens)\n",
        "print(post_train_token_series)\n",
        "\n",
        "post_train_targetLabels_series = pd.Series(post_train_targetLabels)\n",
        "print(post_train_targetLabels_series)"
      ],
      "metadata": {
        "id": "sYaP2tlQQtaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# post test\n",
        "post_test_tokens, post_test_targetLabels = list(), list()\n",
        "\n",
        "post_test_tokens, post_test_targetLabels = tokenize_text(posts_test_df)\n",
        "print(\"post_test_tokens last 5 of dataframe\")\n",
        "print(post_test_tokens[:5])\n",
        "print('post_test_targetLabels last 5 of dataframe')\n",
        "print(post_test_targetLabels[:5])"
      ],
      "metadata": {
        "id": "5yTlFmfWmUIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_test_token_series = pd.Series(post_test_tokens)\n",
        "print(post_test_token_series)\n",
        "\n",
        "post_test_targetLabels_series = pd.Series(post_test_targetLabels)\n",
        "print(post_test_targetLabels_series)"
      ],
      "metadata": {
        "id": "03cTfC39RNN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# post val\n",
        "post_val_tokens, post_val_targetLabels = list(), list()\n",
        "\n",
        "post_val_tokens, post_val_targetLabels = tokenize_text(posts_val_df)\n",
        "print(\"post_val_tokens last 5 of dataframe\")\n",
        "print(post_val_tokens[:5])\n",
        "print('post_val_targetLabels last 5 of dataframe')\n",
        "print(post_val_targetLabels[:5])"
      ],
      "metadata": {
        "id": "029iLdaFmVg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_val_token_series = pd.Series(post_val_tokens)\n",
        "print(post_val_token_series)\n",
        "\n",
        "post_val_targetLabels_series = pd.Series(post_val_targetLabels)\n",
        "print(post_val_targetLabels_series)"
      ],
      "metadata": {
        "id": "h-9N6fnFRn1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(post_train_tokens)"
      ],
      "metadata": {
        "id": "10-MaBKMVrIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(post_test_tokens)"
      ],
      "metadata": {
        "id": "8syj1UMzLQBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(post_val_tokens)"
      ],
      "metadata": {
        "id": "GGtqaFosLREW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class RedditPostsDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings = encodings\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[index])\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ],
      "metadata": {
        "id": "hDYoJk1KGCtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = RedditPostsDataset(post_train_token_series, post_train_targetLabels_series)\n",
        "val_dataset = RedditPostsDataset(post_val_token_series, post_val_targetLabels_series)\n",
        "test_dataset = RedditPostsDataset(post_test_token_series, post_test_targetLabels_series)"
      ],
      "metadata": {
        "id": "bN-ZbfeuIGWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Set Up and Development - Training and Testing"
      ],
      "metadata": {
        "id": "L7PwAyCItJBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# establish training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='outputs/results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "# these are the parameters we will be adjusting to see what is the optimal configuration"
      ],
      "metadata": {
        "id": "9t2PFoW-mhq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5)"
      ],
      "metadata": {
        "id": "HYcP9RHpJSnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up trainer instance with specified attributes\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")"
      ],
      "metadata": {
        "id": "gPhsY3jnJO67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train - might use tqdm module for this to track progress\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "C6I6jFe0JMF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate training\n",
        "trainer.evalute()"
      ],
      "metadata": {
        "id": "JYu1KJvRrZ71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving and Loading DistilBERT for Socrates"
      ],
      "metadata": {
        "id": "U4N-oY_WJemT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = '/saved_models'\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "oFLUXiDuJdHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Pretrained DistilBERT for Inference Testing"
      ],
      "metadata": {
        "id": "zg_-fWGOJ72M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_fine_tuned = DistilBertTokenizer.from_pretrained(save_directory)\n",
        "model_fine_tuned = DistilBertForSequenceClassification.from_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "DBYPXDmNJ6_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = post_test_tokens[0]\n",
        "test_text"
      ],
      "metadata": {
        "id": "2tPhOpEiKdoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "tokenizer_fine_tuned_pt = DistilBertTokenizer.from_pretrained(save_directory)\n",
        "model_fine_tuned_pt = DistilBertForSequenceClassification.from_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "FO8Jf-CtKdh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_input_pt = tokenizer_fine_tuned_pt(test_text,\n",
        "                                           truncation=True,\n",
        "                                           padding=True,\n",
        "                                           return_tensor='pt')\n",
        "\n",
        "output_pt = model_fine_tuned_pt(predict_input_pt)\n",
        "prediction_value_pt = torch.argmax(output_pt[0], dim=1).item()"
      ],
      "metadata": {
        "id": "fMd042xTMxvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_value_pt"
      ],
      "metadata": {
        "id": "76O4DbXOJlvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning with {native} PyTorch/TensorFlow"
      ],
      "metadata": {
        "id": "fEr8Bpi5tO6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DistilBertForSequenceClassification, AdamW"
      ],
      "metadata": {
        "id": "EWFxG5jrzoCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "    for batch in train_loader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "b7D4npQKz11O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "source for distilbert classification: https://huggingface.co/transformers/v3.2.0/custom_datasets.html\n",
        "\n",
        "source for fine-tuning distilbert -- youtube video:\n",
        "https://www.youtube.com/watch?v=ZvsH09XGuZ0\n",
        "\n",
        "huggingface distilbert model training and validation: https://huggingface.co/transformers/v3.0.2/model_doc/distilbert.html\n",
        "\n",
        "fast.ai: https://www.fast.ai/\n"
      ],
      "metadata": {
        "id": "Jn2vLQGc0RbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AMAZON SYNE TUNE: HYPERPARAMETER TUNING JOB (AFTER TRAINING IS FINISHED)**"
      ],
      "metadata": {
        "id": "fXlUVnJbr6gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'syne-tune[extra]'\n",
        "# or get the latest version from git:\n",
        "git clone https://github/com/awslabs/syne-tune.git\n",
        "cd syne-tune\n",
        "python3 -m venv st_venv\n",
        ". st_venv/bin/activate\n",
        "pip install --upgrade pip\n",
        "pip install -e '.[extra]'\n",
        "# remember to activate this environment before working with SyneTune\n",
        "# we are building this venv from scratch now and then, in particular when you pull\n",
        "# a new release, as dependenices may have changed"
      ],
      "metadata": {
        "id": "pzziURSIr56r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_height_simple.py\n",
        "import logging, time\n",
        "from syne_tune import Reporter\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "if __nam__ == \"__main__\":\n",
        "  root = logging.getLogger()\n",
        "  root.setLevel(logging.INFO)\n",
        "  parser = ArgumentParser()\n",
        "  parser.add_argument('--steps', type=int)\n",
        "  parser.add_argument('--width', type=float)\n",
        "  parser.add_argument('--height', type=float)\n",
        "  args, _ = parser.parse_known_args()\n",
        "  report = Reporter()\n",
        "  for step in range(args.steps):\n",
        "    time.sleep(0.1)\n",
        "    dummy_score = 1.9 / (0.1 + args.width * step / 100) + args.height * 0.1\n",
        "    # feed the score back to syne tune\n",
        "    report(epoch=step + 1, mean_loss=dummy_score)"
      ],
      "metadata": {
        "id": "bpZrErRqstvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can launch a tuning job as follows:\n",
        "# launch_height_simple.py\n",
        "from syne_tune import Tuner, StopCriterion\n",
        "from syne_tune.backend import LocalBackend\n",
        "from syne_tune.config_space import randint\n",
        "from syne_tune.optimizer.baselines import ASHA"
      ],
      "metadata": {
        "id": "4W7o47zAtoIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter search space to consider\n",
        "config_space = {\n",
        "    'width': randint(1, 20),\n",
        "    'height': randint(1, 20),\n",
        "    'epochs': 100,\n",
        "}"
      ],
      "metadata": {
        "id": "V_jU3te8-DPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = Tuner(\n",
        "    trial_backend=LocalBackend(entry_point='train_height.py'),\n",
        "    scheduler=ASHA(\n",
        "        config_space,\n",
        "        metric='mean_loss',\n",
        "        resource_attr='epoch',\n",
        "        max_resource_attr='epochs',\n",
        "        search_options={'debug_log': False},\n",
        "    ),\n",
        "    stop_criterion=Stopping(max_wallclock_time=30),\n",
        "    n_workers=4, # how many trials are evaluated in parallel\n",
        ")"
      ],
      "metadata": {
        "id": "nyi-meXs-aIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.run()"
      ],
      "metadata": {
        "id": "9Qa7H0MJ_15y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}